---
title: "assignment07"
author: "Yosup Shin"
format: 
  html:
    embed-resources: true
execute:
  warning: false
  message: false
  error: false
editor: visual
editor_options: 
  chunk_output_type: console
---

## Exercise 1

```{r}
library(tidyverse)
library(lubridate)
library(janitor)

# use this url to download the data directly into R
df <- read_csv("https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv")

# clean names with janitor
sampled_df <- df |> 
  janitor::clean_names() 

# create an inspection year variable
sampled_df <- sampled_df |>
  mutate(inspection_date = mdy(inspection_date)) |>
  mutate(inspection_year = year(inspection_date))

# get most-recent inspection
sampled_df <- sampled_df |>
  group_by(camis) |>
  filter(inspection_date == max(inspection_date)) |>
  ungroup()

# subset the data
sampled_df <- sampled_df |>
  select(camis, boro, zipcode, cuisine_description, inspection_date,
         action, violation_code, violation_description, grade,
         inspection_type, latitude, longitude, council_district,
         census_tract, inspection_year, critical_flag) |>
  drop_na() |>
  filter(inspection_year >= 2017) |>
  filter(grade %in% c("A", "B", "C")) 

# create the binary target variable
sampled_df <- sampled_df |>
  mutate(grade = if_else(grade == "A", "A", "Not A")) |>
  mutate(grade = as.factor(grade))

# create extra predictors
sampled_df <- sampled_df |>
  group_by(boro, zipcode, cuisine_description, inspection_date,
           action, violation_code, violation_description, grade,
           inspection_type, latitude, longitude, council_district,
           census_tract, inspection_year)  |>
  mutate(vermin = str_detect(violation_description, pattern = "mice|rats|vermin|roaches")) |>
  summarize(violations = n(),
            vermin_types = sum(vermin),
            critical_flags = sum(critical_flag == "Y")) |>
  ungroup()

# write the data
write_csv(sampled_df, "restaurant_grades.csv")

## Estimating the model
# set seed
library(tidymodels)
set.seed(20201020)
split <- initial_split(sampled_df, 
                       prop = 0.7, 
                       strata = "grade")

df_train <- training(split)
df_test <- testing(split)

library(themis)

df_rec <- recipe(grade ~ ., data = df_train) |> 
  step_downsample(grade)

library(rpart)
library(rpart.plot)


# 1. Model specification
tree_spec <- decision_tree(mode = "classification") |>
  set_engine("rpart")

# Workflow
tree_wf <- workflow() %>% 
  add_model(tree_spec) %>% 
  add_recipe(df_rec)

# Fit on training data
tree_fit <- fit(tree_wf, data = df_train)

rpart.plot::rpart.plot(x = tree_fit$fit$fit$fit)

## 2. Evaluate the Model
predictions <- bind_cols(
  df_train,
  predict(object = tree_fit, new_data = df_train),
  predict(object = tree_fit, new_data = df_train, type = "prob")
)

select(predictions, grade, starts_with(".pred"))

conf_mat(data = predictions,
         truth = grade,
         estimate = .pred_class,)

precision(data = predictions,
          truth = grade,
          estimate = .pred_class)

recall(data = predictions,
       truth = grade,
       estimate = .pred_class)

# qulity of the model
# Looking at the decision tree, I'd say the model is underfitting and needs improvements since the decision tree is extremely shallow and relies on only one or two splits, which means it cannot capture the complexity of how multiple inspection variables jointly predict restaurant grades.

## 3. Improvement
# The very simple decision tree suggests that the model is underfitting, which could explain why precision is extremely high but recall is noticeably lower.
# Using more flexible algorithms like random forests would capture richer interactions between different variables reducing false negatives. Additionally, improving feature engineering—such as adding interaction terms, creating counts of violation categories, or using tuned hyperparameters like tree depth and minimum split size—would likely improve recall while maintaining strong precision.

## 4. Variable Importance
library(vip)

tree_fit |>
  extract_fit_parsnip() |>
  vip(num_features = 10)

## 5. Application
# The NYC Health Department can predict which establishments are more likely to receive lower grades using an improved and enhanced model. This would allow the department to more effectively allocate inspection resources and promote compliance prior to violations
```

## Exercise 2

```{r}
Chicago_modeling <- Chicago |>
slice(1:5678)

Chicago_implementation <- Chicago |>
slice(5679:5698) |>
select(-ridership)

## 1. Convert data into a useable variable
Chicago_modeling <- Chicago_modeling |>
  mutate(
  weekday = wday(date, label = TRUE),
  month = month(date, label = TRUE),
  yearday = yday(date)
  )

Chicago_implementation <- Chicago_implementation |> 
   mutate(
  weekday = wday(date, label = TRUE),
  month = month(date, label = TRUE),
  yearday = yday(date)
  )

## 2. Set up a testing environment
set.seed(20211101)

Chicago_split <- initial_split(Chicago_modeling)
Chicago_train <- training(Chicago_split)
Chicago_test <- testing(Chicago_split)

# Explanatory data analysis
glimpse(Chicago_train)

Chicago_train |>
  summarise(
    mean_ridership = mean(ridership, na.rm = TRUE),
    minimum_ride = min(ridership, na.rm = TRUE),
    maximum_ride = max(ridership, na.rm = TRUE)
  )

Chicago_train |>
  ggplot(aes(x = month, y = ridership)) +
   geom_boxplot()

Chicago_train |>
  ggplot(aes(x = weekday, y = ridership)) +
   geom_boxplot()

Chicago_folds <- vfold_cv(
  Chicago_train,
  v = 10
)

library(recipes)

Chicago_rec <- recipe(ridership ~ ., data = Chicago_train) |>
  step_holiday(holidays = timeDate::listHolidays("US")) |>
# or step_holiday(all_predictors())
  step_dummy(all_nominal_predictors()) |>
  step_nzv(all_predictors()) |>
  step_zv(all_predictors()) 

# Three models 
library(parsnip)
# Linear regression
lm_chicago <- linear_reg() |>
  set_engine("lm")

lm_wf <- workflow() |>
  add_recipe(Chicago_rec) |>
  add_model(lm_chicago) 

lm_resample <- fit_resamples(
  lm_wf,
  resamples = Chicago_folds,
  metrics = metric_set(rmse)
)

# KNN regression
library(kknn)

knn_Chicago50 <- 
  nearest_neighbor(neighbors = 50) |>
  set_engine(engine = "kknn") |>
  set_mode(mode = "regression") 

knn_wf <- workflow() |>
  add_model(knn_Chicago50) |>
  add_recipe(Chicago_rec)

knn_resample <- fit_resamples(
  knn_wf,
  resamples = Chicago_folds,
  metrics = metric_set(rmse)
)

# Random forest
library(ranger)

rf_Chicago <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 300
) |>
  set_mode("regression") |>
  set_engine("ranger",
             importance = "impurity")

rf_wf <- workflow() |>
  add_model(rf_Chicago) |>
  add_recipe(Chicago_rec)

rf_grid <- grid_regular(
  mtry(range = c(1, 5)),
  min_n(range = c(1, 5)),
  levels = 3
)

rf_resample <- tune_grid(
  rf_wf, 
  resamples = Chicago_folds,
  grid = rf_grid,
  metrics = metric_set(rmse)
)

# Calculate RMSE 

lm_rmse <- collect_metrics(lm_resample, summarize = FALSE) |>
  filter(.metric == "rmse") 

knn_rmse <- collect_metrics(knn_resample, summarize = FALSE) |>
  filter(.metric == "rmse") 

rf_rmse <- collect_metrics(rf_resample, summarize = FALSE) |>
  filter(.metric == "rmse")

lm_mean_rmse  <- mean(lm_rmse$.estimate)
knn_mean_rmse <- mean(knn_rmse$.estimate)
rf_mean_rmse  <- mean(rf_rmse$.estimate)

ggplot(lm_rmse, aes(x = id, y = .estimate)) +
  geom_point() +
  labs(title = "RMSE across folds – Linear regression",
       x = "Fold", y = "RMSE") +
  theme_minimal()

ggplot(knn_rmse, aes(x = id, y = .estimate)) +
  geom_point() +
  labs(title = "RMSE across folds – KNN regression",
       x = "Fold", y = "RMSE") +
  theme_minimal()

ggplot(rf_rmse, aes(x = id, y = .estimate)) +
  geom_point() +
  labs(title = "RMSE across folds – Random Forest",
       x = "Fold", y = "RMSE") +
  theme_minimal()

final_wf <- 
  rf_wf |> 
  finalize_workflow(select_best(rf_resample))

## 4. Estimate the out-of-sample error rate

final_rf_fit <- final_wf |>
  last_fit(Chicago_split) 

test_predictions <- collect_predictions(final_rf_fit)
head(test_predictions)

rmse(test_predictions,
     truth = ridership,
     estimate = .pred)

## 5. Implement the final model
implementation_pred <- predict(extract_workflow(final_rf_fit), Chicago_implementation)

implementation_pred


## 6. Briefly describe the final model
final_rf_vip <- extract_fit_parsnip(final_rf_fit)
vip(final_rf_vip)

# My final model is a tuned random forest it achieved the lowest average RMSE during cross-validation and also produced a low out-of-sample RMSE on the testing set as well. 
# Based on this reesult, the model appears to be a good predictive model that generalizes well to new observations, including the 20 held-out implementation cases. However, they are not globally interpretable, because their predictions combine information from many trees and do not follow a single, simple rule. 
# Nevertheless, they are locally interpretable, so I may look at local explanations (such partial dependence) or variable-importance scores to see which predictors affected a particular prediction.  The weekday and seasonal variables were among the most significant predictors in my model, which is consistent with the strong weekly and seasonal trends in public transportation ridership.  
# The model does well overall in terms of accuracy, but because of its ensemble structure, interpretability is sacrificed.
```
